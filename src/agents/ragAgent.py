from crewai.tools import tool
import agentops
from stores.llms import MakeLLm


class RAGAgentManager:
    def __init__(self, app):
        self.settings = app.settings
        self.qdrant = app.qdrant
        self.cohere_client = app.cohere_client

        LLmOBJ = MakeLLm(self.settings.LLM_ROUTER, self.settings.LLM_TEMP)
        self.llm = LLmOBJ.getLLm()

        self.summary_memory = "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªÙ„Ø®ÙŠØµ Ø¨Ø¹Ø¯."

        agentops.init(
            api_key=self.settings.AGENTOPS_API_KEY,
            skip_auto_end_session=True
        )

    def kb_tool(self, query: str) -> str:
        try:
            if not self.qdrant.isCollectionExisted("rag_data"):
                return "Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø±ÙØ´ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¹Ù† Ø³Ø¤Ø§Ù„Ùƒ."

            response = self.cohere_client.embed(
                model="embed-multilingual-v3.0",
                texts=[query],
                input_type="search_query"
            )
            query_vector = response.embeddings[0]

            results = self.qdrant.searchByVector(
                collectionName="rag_data",
                vector=query_vector,
                limit=5
            )

            if not results:
                return "Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø±ÙØ´ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¹Ù† Ø³Ø¤Ø§Ù„Ùƒ."

            retrieved_texts = [doc.text for doc in results]
            return "\n\n".join(retrieved_texts)

        except Exception as e:
            print(f"âŒ Error in knowledge_base_search: {e}")
            return "Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø±ÙØ´ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¹Ù† Ø³Ø¤Ø§Ù„Ùƒ."

    def build_prompt(self, query: str, context_docs: str) -> str:
        prompt = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ. Ø¬Ø§ÙˆØ¨ Ø¨Ø¯Ù‚Ø© Ø§Ø¹ØªÙ…Ø§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
        Ù„Ùˆ Ù…ÙÙŠØ´ Ù…Ø¹Ù„ÙˆÙ…Ø© ÙƒØ§ÙÙŠØ© Ø¬Ø§ÙˆØ¨: "Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø±ÙØ´ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¹Ù† Ø³Ø¤Ø§Ù„Ùƒ."

        ğŸ§  Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø³Ø§Ø¨Ù‚:
        {self.summary_memory}

        ğŸ§© Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:
        {context_docs}

        â“ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ:
        {query}

        âœï¸ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        """
        return prompt.strip()

    def update_summary(self, query: str, answer: str):
        try:
            summarization_prompt = f"""
            Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:
            {self.summary_memory}

            Ø¢Ø®Ø± ØªÙØ§Ø¹Ù„:
            - Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {query}
            - Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {answer}

            Ø±Ø¬Ù‘Ø¹ ØªÙ„Ø®ÙŠØµ Ø¬Ø¯ÙŠØ¯ Ù…Ø®ØªØµØ± ÙˆØ¯Ù‚ÙŠÙ‚ ÙŠØ¶Ù… Ø£Ù‡Ù… Ø§Ù„Ù†Ù‚Ø§Ø· ÙÙ‚Ø·.
            """
            response = self.llm.call(summarization_prompt)

            if isinstance(response, dict) and "text" in response:
                self.summary_memory = response["text"].strip()
            else:
                self.summary_memory = str(response).strip()
        except Exception as e:
            print(f"âŒ Error updating summary: {e}")

    def ask(self, query: str) -> str:
        try:
            context_docs = self.kb_tool(query)
            if "Ù„Ù„Ø£Ø³ÙØŒ Ù…Ø¹Ø±ÙØ´" in context_docs:
                return context_docs

            prompt = self.build_prompt(query, context_docs)
            response = self.llm.call(prompt)

            if isinstance(response, dict) and "text" in response:
                answer = response["text"]
            else:
                answer = str(response).strip()

            self.update_summary(query, answer)

            return answer

        except Exception as e:
            print(f"âŒ Error in RAG ask(): {e}")
            return "Ø­ØµÙ„ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©."
